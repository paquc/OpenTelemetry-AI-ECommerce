services:

  servemodelapi:
    build:
      context: ./servemodelfastapi
      dockerfile: Dockerfile
    container_name: servemodel_fast_api
    ports:
      - "8088:8088"
    deploy:
      resources:
        reservations:
          memory: 64g  
    volumes:
      - ./servemodelfastapi/main.py:/main.py
      - ./servemodelfastapi/data/:/data/

  apigateway:
    build:
      context: .
      dockerfile: Dockerfile.apigateway
    volumes:
      - ./apigateway/apigateway.js:/app/apigateway.js
      - ./apigateway/.env:/app/.env
      - ./logstash_ingest_data/:/usr/share/logstash/ingest_data/
      - ./filebeat_ingest_data/:/usr/share/filebeat/ingest_data/
    container_name: apigateway
    ports:
      - "9000:9000"

  influxdb:
    image: influxdb:1.8
    container_name: influxdb
    ports:
      - "8086:8086"
    environment:
      - INFLUXDB_DB=k6

  k6:
    image: grafana/k6:0.55.0
    container_name: k6  
    ports:
      - "6565:6565"
    environment:
      - K6_OUT=influxdb=http://influxdb:8086/k6
    volumes:
      - ./examples:/scripts

  splunk:
    image: splunk/splunk:9.3
    container_name: splunk
    environment:
      - SPLUNK_START_ARGS=--accept-license
      - SPLUNK_PASSWORD=WL84MySplunk!       # admnin / WL84MySplunk!
    ports:
      - "8001:8000"
      #- "8088:8088"
      #- "8089:8089"
      #- "9997:9997"
    # restart: on-failure
         
  base:
    build:
      context: .
      dockerfile: Dockerfile.base 
    restart: on-failure
    environment:
      - PG=true

  lb:
    image: dockercloud/haproxy
    links:
      - admin
      - end-user
      - monitoring
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    ports:
      - 80:80
      - 443:443
    restart: on-failure

  admin:
    build:
      context: .
      dockerfile: Dockerfile.admin
    command: node -r ./admin/tracing.js admin/server.js 
    environment:
      - VIRTUAL_HOST=local-admin.cotejs.org, ws://local-admin.cotejs.org
      - BALANCE=source
      - PG=true
    volumes:
      - ./tracing/tracing.js:/src/admin/tracing.js
      - ./admin/server.js:/src/admin/server.js
    restart: on-failure
    ports:
      - 5000:5000

  end-user:
    build:
      context: .
      dockerfile: Dockerfile.enduser
    command: node -r ./end-user/tracing.js end-user/server.js 
    environment:
      - VIRTUAL_HOST=local-end-user.cotejs.org, ws://local-end-user.cotejs.org
      - BALANCE=source
      - PG=true
    volumes:
      - ./tracing/tracing.js:/src/end-user/tracing.js
      - ./end-user/server.js:/src/end-user/server.js
      - ./end-user/index.html:/src/end-user/index.html
    restart: on-failure
    ports:
      - 5001:5001

  monitoring:
    extends: base
    command: node monitor.js
    environment:
      - PORT=80
      - VIRTUAL_HOST=local-monitoring.cotejs.org
    restart: on-failure
    ports:
      - 5555:5555

  payment-service:
    build:
      context: .
      dockerfile: Dockerfile.payment
    command: node -r ./payment/tracing.js payment/payment-service.js
    volumes:
      - ./tracing/tracing.js:/src/payment/tracing.js
      - ./payment/payment-service.js:/src/payment/payment-service.js
    environment:
      - PG=true
    restart: on-failure
    depends_on:
      - db

  product-service:
    build:
      context: .
      dockerfile: Dockerfile.product
    command: node -r ./product/tracing.js product/product-service.js
    volumes:
      - ./tracing/tracing.js:/src/product/tracing.js
      - ./product/product-service.js:/src/product/product-service.js
    environment:
      - PG=true
    restart: on-failure
    depends_on:
      - db

  purchase-service:
    build:
      context: .
      dockerfile: Dockerfile.purchase
    command: node -r ./purchase/tracing.js purchase/purchase-service.js
    volumes:
      - ./tracing/tracing.js:/src/purchase/tracing.js
      - ./purchase/purchase-service.js:/src/purchase/purchase-service.js
    environment:
      - PG=true
    restart: on-failure
    depends_on:
      - db

  user-service:
    build:
      context: .
      dockerfile: Dockerfile.user
    command: node -r ./user/tracing.js user/user-service.js
    volumes:
      - ./tracing/tracing.js:/src/user/tracing.js
      - ./user/user-service.js:/src/user/user-service.js
    environment:
      - PG=true
    restart: on-failure
    depends_on:
      - db

  db:
    extends: base
    command: node init-db.js
    depends_on:
      - postgres
      - base

  postgres:
    image: sameersbn/postgresql:9.6-1
    restart: on-failure
    environment:
      - DB_USER=cote
      - DB_PASS=ohgath2ig8eoP8
      - DB_NAME=cote

  # Collector
  collector-gateway:
    image: otel/opentelemetry-collector:0.110.0
    container_name: otel-collector-gateway
    volumes:
      - ./collector-gateway.yaml:/etc/collector-gateway.yaml
    command: [ "--config=/etc/collector-gateway.yaml" ]
    #restart: on-failure
    ports:
      - "1888:1888"         # pprof extension
      - "8888:8888"         # Prometheus metrics: exposed by the Collector to monitor the health of the collector itself
      - "8889:8889"         # Prometheus EXPORTER metrics: endpoint where services metrics are sent then scraped by the prometheus service to monitor instrumented services.
      - "13133:13133"       # health_check extension
      - "4317:4317"         # OTLP gRPC receiver
      - "4318:4318"         # OTLP HTTP receiver
      - "55679:55679"       # zpages extension

  # Loki to collect logs
  loki:
    image: grafana/loki:main-f80d68a
    container_name: loki
    ports:
      - "3100:3100"        # HTTP port to receive logs data from the OTEL Collector
    restart: on-failure
    volumes:
      - ./loki/loki-config.yaml:/etc/loki/loki-config.yaml
    command: [ "--config.file=/etc/loki/loki-config.yaml" ]
     
  prometheus:
    build:
      context: ./prometheus
      dockerfile: Dockerfile
    container_name: prometheus
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'  
      - '--query.lookback-delta=5s'
      - '--web.enable-remote-write-receiver'  # Enable remote write receiver
    ports:
      - "9090:9090"
    restart: on-failure
    depends_on:
      - collector-gateway
  
  grafana:
    image: grafana/grafana-enterprise # :10.3.10
    container_name: grafana
    restart: unless-stopped
    ports:
      - "3010:3000"
    volumes:
      - ./grafana-storage:/var/lib/grafana

  # Tempo runs as user 10001, and docker compose creates the volume as root.
  # As such, we need to chown the volume in order for Tempo to start correctly.
  init:
    image: &tempoImage grafana/tempo:2.6.0
    container_name: inittempo
    user: root
    entrypoint:
      - "chown"
      - "10001:10001"
      - "/var/tempo"
    volumes:
      - ./tempo-data:/var/tempo

  tempo:
    image: *tempoImage
    container_name: tempo
    command: [ "-config.file=/etc/tempo.yaml" ]
    volumes:
      - ./tempo/tempo.yaml:/etc/tempo.yaml
      - ./tempo-data:/var/tempo
    ports:
      - "14268"   # jaeger ingest
      - "3200"    # tempo
      - "4317"    # otlp grpc
      - "4318"    # otlp http
      - "9411"    # zipkin
    depends_on:
      - init
   
  setup:
    image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}
    container_name: setup_elk
    volumes:
      - certs:/usr/share/elasticsearch/config/certs
    user: "0"
    command: >
      bash -c '
        if [ x${ELASTIC_PASSWORD} == x ]; then
          echo "Set the ELASTIC_PASSWORD environment variable in the .env file";
          exit 1;
        elif [ x${KIBANA_PASSWORD} == x ]; then
          echo "Set the KIBANA_PASSWORD environment variable in the .env file";
          exit 1;
        fi;
        if [ ! -f config/certs/ca.zip ]; then
          echo "Creating CA";
          bin/elasticsearch-certutil ca --silent --pem -out config/certs/ca.zip;
          unzip config/certs/ca.zip -d config/certs;
        fi;
        if [ ! -f config/certs/certs.zip ]; then
          echo "Creating certs";
          echo -ne \
          "instances:\n"\
          "  - name: es01\n"\
          "    dns:\n"\
          "      - es01\n"\
          "      - localhost\n"\
          "    ip:\n"\
          "      - 127.0.0.1\n"\
          "  - name: kibana\n"\
          "    dns:\n"\
          "      - kibana\n"\
          "      - localhost\n"\
          "    ip:\n"\
          "      - 127.0.0.1\n"\
          "  - name: fleet-server\n"\
          "    dns:\n"\
          "      - fleet-server\n"\
          "      - localhost\n"\
          "    ip:\n"\
          "      - 127.0.0.1\n"\
          > config/certs/instances.yml;
          bin/elasticsearch-certutil cert --silent --pem -out config/certs/certs.zip --in config/certs/instances.yml --ca-cert config/certs/ca/ca.crt --ca-key config/certs/ca/ca.key;
          unzip config/certs/certs.zip -d config/certs;
        fi;
        echo "Setting file permissions"
        chown -R root:root config/certs;
        find . -type d -exec chmod 750 \{\} \;;
        find . -type f -exec chmod 640 \{\} \;;
        echo "Waiting for Elasticsearch availability";
        until curl -s --cacert config/certs/ca/ca.crt https://es01:9200 | grep -q "missing authentication credentials"; do sleep 30; done;
        echo "Setting kibana_system password";
        until curl -s -X POST --cacert config/certs/ca/ca.crt -u "elastic:${ELASTIC_PASSWORD}" -H "Content-Type: application/json" https://es01:9200/_security/user/kibana_system/_password -d "{\"password\":\"${KIBANA_PASSWORD}\"}" | grep -q "^{}"; do sleep 10; done;
        echo "All done!";
      '
    healthcheck:
      test: ["CMD-SHELL", "[ -f config/certs/es01/es01.crt ]"]
      interval: 1s
      timeout: 5s
      retries: 120

  es01:
    depends_on:
      setup:
        condition: service_healthy
    image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}
    container_name: es01
    labels:
      co.elastic.logs/module: elasticsearch
    volumes:
      - certs:/usr/share/elasticsearch/config/certs
      - esdata01:/usr/share/elasticsearch/data
    ports:
      - ${ES_PORT}:9200
    environment:
      - node.name=es01
      - cluster.name=${CLUSTER_NAME}
      - discovery.type=single-node
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
      - bootstrap.memory_lock=true
      - xpack.security.enabled=true
      - xpack.security.http.ssl.enabled=true
      - xpack.security.http.ssl.key=certs/es01/es01.key
      - xpack.security.http.ssl.certificate=certs/es01/es01.crt
      - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.enabled=true
      - xpack.security.transport.ssl.key=certs/es01/es01.key
      - xpack.security.transport.ssl.certificate=certs/es01/es01.crt
      - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.verification_mode=certificate
      - xpack.license.self_generated.type=${LICENSE}
    mem_limit: ${ES_MEM_LIMIT}
    ulimits:
      memlock:
        soft: -1
        hard: -1
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "curl -s --cacert config/certs/ca/ca.crt https://localhost:9200 | grep -q 'missing authentication credentials'",
        ]
      interval: 10s
      timeout: 10s
      retries: 120

  kibana:
    depends_on:
      es01:
        condition: service_healthy
    image: docker.elastic.co/kibana/kibana:${STACK_VERSION}
    container_name: kibana
    labels:
      co.elastic.logs/module: kibana
    volumes:
      - certs:/usr/share/kibana/config/certs
      - kibanadata:/usr/share/kibana/data
      - ./kibana.yml:/usr/share/kibana/config/kibana.yml:ro
    ports:
      - ${KIBANA_PORT}:5601
    environment:
      - SERVERNAME=kibana
      - ELASTICSEARCH_HOSTS=https://es01:9200
      - ELASTICSEARCH_USERNAME=kibana_system
      - ELASTICSEARCH_PASSWORD=${KIBANA_PASSWORD}
      - ELASTICSEARCH_SSL_CERTIFICATEAUTHORITIES=config/certs/ca/ca.crt
      - XPACK_SECURITY_ENCRYPTIONKEY=${ENCRYPTION_KEY}
      - XPACK_ENCRYPTEDSAVEDOBJECTS_ENCRYPTIONKEY=${ENCRYPTION_KEY}
      - XPACK_REPORTING_ENCRYPTIONKEY=${ENCRYPTION_KEY}
      - XPACK_REPORTING_KIBANASERVER_HOSTNAME=localhost
      - SERVER_SSL_ENABLED=true
      - SERVER_SSL_CERTIFICATE=config/certs/kibana/kibana.crt
      - SERVER_SSL_KEY=config/certs/kibana/kibana.key
      - SERVER_SSL_CERTIFICATEAUTHORITIES=config/certs/ca/ca.crt
      - ELASTIC_APM_SECRET_TOKEN=${ELASTIC_APM_SECRET_TOKEN}
    mem_limit: ${KB_MEM_LIMIT}
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "curl -I -s --cacert config/certs/ca/ca.crt https://localhost:5601 | grep -q 'HTTP/1.1 302 Found'",
        ]
      interval: 10s
      timeout: 10s
      retries: 120

  metricbeat01:
    depends_on:
      es01:
        condition: service_healthy
      kibana:
        condition: service_healthy
    image: docker.elastic.co/beats/metricbeat:${STACK_VERSION}
    container_name: metricbeat01
    user: root
    volumes:
      - certs:/usr/share/metricbeat/certs
      - metricbeatdata01:/usr/share/metricbeat/data
      - "./metricbeat.yml:/usr/share/metricbeat/metricbeat.yml:ro"
      - "/var/run/docker.sock:/var/run/docker.sock:ro"
      - "/sys/fs/cgroup:/hostfs/sys/fs/cgroup:ro"
      - "/proc:/hostfs/proc:ro"
      - "/:/hostfs:ro"
    environment:
      - ELASTIC_USER=elastic
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
      - ELASTIC_HOSTS=https://es01:9200
      - KIBANA_HOSTS=https://kibana:5601
      - LOGSTASH_HOSTS=http://logstash01:9600
      - CA_CERT=certs/ca/ca.crt
      - ES_CERT=certs/es01/es01.crt
      - ES_KEY=certs/es01/es01.key
      - KB_CERT=certs/kibana/kibana.crt
      - KB_KEY=certs/kibana/kibana.key
    command:
      -strict.perms=false

  filebeat01:
    depends_on:
      es01:
        condition: service_healthy
    image: docker.elastic.co/beats/filebeat:${STACK_VERSION}
    container_name: filebeat01
    user: root
    volumes:
      - certs:/usr/share/filebeat/certs
      - filebeatdata01:/usr/share/filebeat/data
      - "./filebeat_ingest_data/:/usr/share/filebeat/ingest_data/"
      - "./filebeat.yml:/usr/share/filebeat/filebeat.yml:ro"
      - "/var/lib/docker/containers:/var/lib/docker/containers:ro"
      - "/var/run/docker.sock:/var/run/docker.sock:ro"
    environment:
      - ELASTIC_USER=elastic
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
      - ELASTIC_HOSTS=https://es01:9200
      - KIBANA_HOSTS=https://kibana:5601
      - LOGSTASH_HOSTS=http://logstash01:9600
      - CA_CERT=certs/ca/ca.crt
    command:
      -strict.perms=false

  logstash01:
    depends_on:
      es01:
        condition: service_healthy
      kibana:
        condition: service_healthy
    image: docker.elastic.co/logstash/logstash:${STACK_VERSION}
    container_name: logstash01
    labels:
      co.elastic.logs/module: logstash
    user: root
    volumes:
      - certs:/usr/share/logstash/certs
      - logstashdata01:/usr/share/logstash/data
      - "./logstash_ingest_data/:/usr/share/logstash/ingest_data/"
      - "./logstash.conf:/usr/share/logstash/pipeline/logstash.conf:ro"
    environment:
      - xpack.monitoring.enabled=false
      - ELASTIC_USER=elastic
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
      - ELASTIC_HOSTS=https://es01:9200

  fleet-server:
    depends_on:
      kibana:
        condition: service_healthy
      es01:
        condition: service_healthy
    image: docker.elastic.co/beats/elastic-agent:${STACK_VERSION}
    container_name: fleet-server
    volumes:
      - certs:/certs
      - fleetserverdata:/usr/share/elastic-agent
      - "/var/lib/docker/containers:/var/lib/docker/containers:ro"
      - "/var/run/docker.sock:/var/run/docker.sock:ro"
      - "/sys/fs/cgroup:/hostfs/sys/fs/cgroup:ro"
      - "/proc:/hostfs/proc:ro"
      - "/:/hostfs:ro"
    ports:
      - ${FLEET_PORT}:8220
      - ${APMSERVER_PORT}:8200
    user: root
    environment:
      - SSL_CERTIFICATE_AUTHORITIES=/certs/ca/ca.crt
      - CERTIFICATE_AUTHORITIES=/certs/ca/ca.crt
      - FLEET_CA=/certs/ca/ca.crt
      - FLEET_ENROLL=1
      - FLEET_INSECURE=true
      - FLEET_SERVER_ELASTICSEARCH_CA=/certs/ca/ca.crt
      - FLEET_SERVER_ELASTICSEARCH_HOST=https://es01:9200
      - FLEET_SERVER_ELASTICSEARCH_INSECURE=true
      - FLEET_SERVER_ENABLE=1
      - FLEET_SERVER_CERT=/certs/fleet-server/fleet-server.crt
      - FLEET_SERVER_CERT_KEY=/certs/fleet-server/fleet-server.key
      - FLEET_SERVER_INSECURE_HTTP=true
      - FLEET_SERVER_POLICY_ID=fleet-server-policy
      - FLEET_URL=https://fleet-server:8220
      - KIBANA_FLEET_CA=/certs/ca/ca.crt
      - KIBANA_FLEET_SETUP=1
      - KIBANA_FLEET_USERNAME=elastic
      - KIBANA_FLEET_PASSWORD=${ELASTIC_PASSWORD}
      - KIBANA_HOST=https://kibana:5601

  webapp:
    build:
      context: app
    volumes:
      - "/var/lib/docker/containers:/var/lib/docker/containers:ro"
      - "/var/run/docker.sock:/var/run/docker.sock:ro"
      - "/sys/fs/cgroup:/hostfs/sys/fs/cgroup:ro"
      - "/proc:/hostfs/proc:ro"
      - "/:/hostfs:ro"
    ports:
      - 8010:8000    
    
  #mimir:
  #  image: grafana/mimir:2.13.0
  #  container_name: mimir
  # command: ["-ingester.native-histograms-ingestion-enabled=true", "-config.file=/etc/mimir.yaml"]
  #  ports:
  #    - "9009:9009"
  #  volumes:
  #    - "./mimir/mimir.yaml:/etc/mimir.yaml"  

  # Generate fake traces...
  # k6-tracing:
  #   image: ghcr.io/grafana/xk6-client-tracing:v0.0.5
  #   container_name: k6
  #   environment:
  #     - ENDPOINT=collector-gateway:4317 # Export to OTEL Collector simulatated traces
  #   #restart: always
  #   depends_on:
  #     - collector-gateway  

  # docker run --name logparser -it -v logparser:/logparser logpai/logparser:py3 bash
  # docker run --name logparser -it -v logparser:/logparser logparser_py3_8:1.0 bash
  # logparser:
  #   build:
  #     context: ./logparserDocker
  #     dockerfile: Dockerfile
  #   container_name: logparser_py38
  #   deploy:
  #     resources:
  #       reservations:
  #         memory: 64g  
  #   volumes:
  #     - logparserpy38:/logparser
  #   stdin_open: true        # Keep STDIN open for interaction
  #   tty: true               # Allocate a pseudo-TTY
  #   command: /bin/bash      # Start the container with Bash

volumes:
  certs:
    driver: local
  esdata01:
    driver: local
  kibanadata:
    driver: local
  metricbeatdata01:
    driver: local
  filebeatdata01:
    driver: local
  logstashdata01:
    driver: local
  fleetserverdata:
    driver: local

networks:
  default:
    name: elastic
    external: false