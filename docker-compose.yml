services:
        
  splunk:
    image: splunk/splunk:9.3
    container_name: splunk
    environment:
      - SPLUNK_START_ARGS=--accept-license
      - SPLUNK_PASSWORD=WL84MySplunk!       # admnin / WL84MySplunk!
    ports:
      - "8001:8000"
      #- "8088:8088"
      #- "8089:8089"
      #- "9997:9997"
    # restart: on-failure
         
  base:
    build:
      context: .
      dockerfile: Dockerfile.base 
    restart: on-failure
    environment:
      - PG=true

  lb:
    image: dockercloud/haproxy
    links:
      - admin
      - end-user
      - monitoring
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    ports:
      - 80:80
      - 443:443
    restart: on-failure

  admin:
    build:
      context: .
      dockerfile: Dockerfile.admin
    command: node -r ./admin/tracing.js admin/server.js 
    environment:
      - VIRTUAL_HOST=local-admin.cotejs.org, ws://local-admin.cotejs.org
      - BALANCE=source
      - PG=true
    volumes:
      - ./tracing/tracing.js:/src/admin/tracing.js
      - ./admin/server.js:/src/admin/server.js
    restart: on-failure
    ports:
      - 5000:5000

  end-user:
    build:
      context: .
      dockerfile: Dockerfile.enduser
    command: node -r ./end-user/tracing.js end-user/server.js 
    environment:
      - VIRTUAL_HOST=local-end-user.cotejs.org, ws://local-end-user.cotejs.org
      - BALANCE=source
      - PG=true
    volumes:
      - ./tracing/tracing.js:/src/end-user/tracing.js
      - ./end-user/server.js:/src/end-user/server.js
      - ./end-user/index.html:/src/end-user/index.html
    restart: on-failure
    ports:
      - 5001:5001

  monitoring:
    extends: base
    command: node monitor.js
    environment:
      - PORT=80
      - VIRTUAL_HOST=local-monitoring.cotejs.org
    restart: on-failure
    ports:
      - 5555:5555

  payment-service:
    build:
      context: .
      dockerfile: Dockerfile.payment
    command: node -r ./payment/tracing.js payment/payment-service.js
    volumes:
      - ./tracing/tracing.js:/src/payment/tracing.js
      - ./payment/payment-service.js:/src/payment/payment-service.js
    environment:
      - PG=true
    restart: on-failure
    depends_on:
      - db

  product-service:
    build:
      context: .
      dockerfile: Dockerfile.product
    command: node -r ./product/tracing.js product/product-service.js
    volumes:
      - ./tracing/tracing.js:/src/product/tracing.js
      - ./product/product-service.js:/src/product/product-service.js
    environment:
      - PG=true
    restart: on-failure
    depends_on:
      - db

  purchase-service:
    build:
      context: .
      dockerfile: Dockerfile.purchase
    command: node -r ./purchase/tracing.js purchase/purchase-service.js
    volumes:
      - ./tracing/tracing.js:/src/purchase/tracing.js
      - ./purchase/purchase-service.js:/src/purchase/purchase-service.js
    environment:
      - PG=true
    restart: on-failure
    depends_on:
      - db

  user-service:
    build:
      context: .
      dockerfile: Dockerfile.user
    command: node -r ./user/tracing.js user/user-service.js
    volumes:
      - ./tracing/tracing.js:/src/user/tracing.js
      - ./user/user-service.js:/src/user/user-service.js
    environment:
      - PG=true
    restart: on-failure
    depends_on:
      - db

  db:
    extends: base
    command: node init-db.js
    depends_on:
      - postgres
      - base

  postgres:
    image: sameersbn/postgresql:9.6-1
    restart: on-failure
    environment:
      - DB_USER=cote
      - DB_PASS=ohgath2ig8eoP8
      - DB_NAME=cote

  # Collector
  collector-gateway:
    image: otel/opentelemetry-collector:0.110.0
    container_name: otel-collector-gateway
    volumes:
      - ./collector-gateway.yaml:/etc/collector-gateway.yaml
    command: [ "--config=/etc/collector-gateway.yaml" ]
    #restart: on-failure
    ports:
      - "1888:1888"         # pprof extension
      - "8888:8888"         # Prometheus metrics: exposed by the Collector to monitor the health of the collector itself
      - "8889:8889"         # Prometheus EXPORTER metrics: endpoint where services metrics are sent then scraped by the prometheus service to monitor instrumented services.
      - "13133:13133"       # health_check extension
      - "4317:4317"         # OTLP gRPC receiver
      - "4318:4318"         # OTLP HTTP receiver
      - "55679:55679"       # zpages extension

  # Loki to collect logs
  loki:
    image: grafana/loki:main-f80d68a
    container_name: loki
    ports:
      - "3100:3100"        # HTTP port to receive logs data from the OTEL Collector
    restart: on-failure
    volumes:
      - ./loki/loki-config.yaml:/etc/loki/loki-config.yaml
    command: [ "--config.file=/etc/loki/loki-config.yaml" ]
     
  prometheus:
    build:
      context: ./prometheus
      dockerfile: Dockerfile
    container_name: prometheus
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'  
      - '--query.lookback-delta=5s'
      - '--web.enable-remote-write-receiver'  # Enable remote write receiver
    ports:
      - "9090:9090"
    restart: on-failure
    depends_on:
      - collector-gateway
  
  grafana:
    image: grafana/grafana-enterprise # :10.3.10
    container_name: grafana
    restart: unless-stopped
    ports:
      - "3010:3000"
    volumes:
      - ./grafana-storage:/var/lib/grafana

  # Tempo runs as user 10001, and docker compose creates the volume as root.
  # As such, we need to chown the volume in order for Tempo to start correctly.
  init:
    image: &tempoImage grafana/tempo:2.6.0
    container_name: inittempo
    user: root
    entrypoint:
      - "chown"
      - "10001:10001"
      - "/var/tempo"
    volumes:
      - ./tempo-data:/var/tempo

  tempo:
    image: *tempoImage
    container_name: tempo
    command: [ "-config.file=/etc/tempo.yaml" ]
    volumes:
      - ./tempo/tempo.yaml:/etc/tempo.yaml
      - ./tempo-data:/var/tempo
    ports:
      - "14268"   # jaeger ingest
      - "3200"    # tempo
      - "4317"    # otlp grpc
      - "4318"    # otlp http
      - "9411"    # zipkin
    depends_on:
      - init
    
  # docker exec -it elasticsearch /usr/share/elasticsearch/bin/elasticsearch-reset-password -u elastic
  # docker exec -it elasticsearch /usr/share/elasticsearch/bin/elasticsearch-create-enrollment-token -s kibana  
  # export ELASTIC_PASSWORD="lZ*4J8_EQZOplAkb+=Fe"
  # docker cp elasticsearch:/usr/share/elasticsearch/config/certs/http_ca.crt .
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.15.2
    container_name: elasticsearch
    restart: always
    environment:
      - discovery.type=single-node
      - ES_JAVA_OPTS=-Xms256m -Xmx256m
    volumes:
      - es_data:/usr/share/elasticsearch/data
    deploy:
      resources:
        reservations:
          memory: 1GB  
    ports:
      - "9200:9200"
      - "9300:9300"

  logstash:
    image: docker.elastic.co/logstash/logstash:8.15.2
    container_name: logstash
    # restart: always
    environment:
      - LS_JAVA_OPTS=-Xms256m -Xmx256m
    volumes:
      - ./logstash/config:/usr/share/logstash/config
      - ./logstash/pipeline:/usr/share/logstash/pipeline
    command: logstash -f /usr/share/logstash/pipeline/logstash.conf
    ports:
      - "9600:9600"
      - "9700:9700"
      #- "5044:5044"
    depends_on:
      - elasticsearch
    
  kibana:
    image: docker.elastic.co/kibana/kibana:8.15.2
    container_name: kibana
    restart: always
    environment:
      - ELASTICSEARCH_URL=http://elasticsearch:9200
    ports:
      - "5601:5601"
    depends_on:
      - elasticsearch
    
    
  #mimir:
  #  image: grafana/mimir:2.13.0
  #  container_name: mimir
  # command: ["-ingester.native-histograms-ingestion-enabled=true", "-config.file=/etc/mimir.yaml"]
  #  ports:
  #    - "9009:9009"
  #  volumes:
  #    - "./mimir/mimir.yaml:/etc/mimir.yaml"  

  # Generate fake traces...
  # k6-tracing:
  #   image: ghcr.io/grafana/xk6-client-tracing:v0.0.5
  #   container_name: k6
  #   environment:
  #     - ENDPOINT=collector-gateway:4317 # Export to OTEL Collector simulatated traces
  #   #restart: always
  #   depends_on:
  #     - collector-gateway  

  # docker run --name logparser -it -v logparser:/logparser logpai/logparser:py3 bash
  # docker run --name logparser -it -v logparser:/logparser logparser_py3_8:1.0 bash
  # logparser:
  #   build:
  #     context: ./logparser
  #     dockerfile: Dockerfile
  #   container_name: logparser_py38
  #   deploy:
  #     resources:
  #       reservations:
  #         memory: 64g  
  #   volumes:
  #     - logparserpy38:/logparser
  #   stdin_open: true        # Keep STDIN open for interaction
  #   tty: true               # Allocate a pseudo-TTY
  #   command: /bin/bash      # Start the container with Bash

volumes:
  es_data:
    driver: local
  #logparserpy38:

# networks:
#  elk:
#    driver: bridge