services:
  
  base:
    build:
      context: .
      dockerfile: Dockerfile.base 
    restart: on-failure
    environment:
      - PG=true

  lb:
    image: dockercloud/haproxy
    links:
      - admin
      - end-user
      - monitoring
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    ports:
      - 80:80
      - 443:443
    restart: on-failure

  admin:
    build:
      context: .
      dockerfile: Dockerfile.admin
    command: node -r ./admin/tracing.js admin/server.js 
    environment:
      - VIRTUAL_HOST=local-admin.cotejs.org, ws://local-admin.cotejs.org
      - BALANCE=source
      - PG=true
    volumes:
      - ./tracing/tracing.js:/src/admin/tracing.js
      - ./admin/server.js:/src/admin/server.js
    restart: on-failure
    ports:
      - 5000:5000

  end-user:
    build:
      context: .
      dockerfile: Dockerfile.enduser
    command: node -r ./end-user/tracing.js end-user/server.js 
    environment:
      - VIRTUAL_HOST=local-end-user.cotejs.org, ws://local-end-user.cotejs.org
      - BALANCE=source
      - PG=true
    volumes:
      - ./tracing/tracing.js:/src/end-user/tracing.js
      - ./end-user/server.js:/src/end-user/server.js
    restart: on-failure
    ports:
      - 5001:5001

  monitoring:
    extends: base
    command: node monitor.js
    environment:
      - PORT=80
      - VIRTUAL_HOST=local-monitoring.cotejs.org
    restart: on-failure
    ports:
      - 5555:5555

  payment-service:
    build:
      context: .
      dockerfile: Dockerfile.payment
    command: node -r ./payment/tracing.js payment/payment-service.js
    volumes:
      - ./tracing/tracing.js:/src/payment/tracing.js
      - ./payment/payment-service.js:/src/payment/payment-service.js
    environment:
      - PG=true
    restart: on-failure
    depends_on:
      - db

  product-service:
    build:
      context: .
      dockerfile: Dockerfile.product
    command: node -r ./product/tracing.js product/product-service.js
    volumes:
      - ./tracing/tracing.js:/src/product/tracing.js
      - ./product/product-service.js:/src/product/product-service.js
    environment:
      - PG=true
    restart: on-failure
    depends_on:
      - db

  purchase-service:
    build:
      context: .
      dockerfile: Dockerfile.purchase
    command: node -r ./purchase/tracing.js purchase/purchase-service.js
    volumes:
      - ./tracing/tracing.js:/src/purchase/tracing.js
      - ./purchase/purchase-service.js:/src/purchase/purchase-service.js
    environment:
      - PG=true
    restart: on-failure
    depends_on:
      - db

  user-service:
    build:
      context: .
      dockerfile: Dockerfile.user
    command: node -r ./user/tracing.js user/user-service.js
    volumes:
      - ./tracing/tracing.js:/src/user/tracing.js
      - ./user/user-service.js:/src/user/user-service.js
    environment:
      - PG=true
    restart: on-failure
    depends_on:
      - db

  db:
    extends: base
    command: node init-db.js
    depends_on:
      - postgres
      - base

  postgres:
    image: sameersbn/postgresql:9.6-1
    restart: on-failure
    environment:
      - DB_USER=cote
      - DB_PASS=ohgath2ig8eoP8
      - DB_NAME=cote

  # Collector
  collector-gateway:
    image: otel/opentelemetry-collector:0.110.0
    container_name: otel-collector-gateway
    volumes:
      - ./collector-gateway.yaml:/etc/collector-gateway.yaml
    command: [ "--config=/etc/collector-gateway.yaml" ]
    restart: on-failure
    ports:
      - "1888:1888"         # pprof extension
      - "8888:8888"         # Prometheus metrics: exposed by the Collector to monitor the health of the collector itself
      - "8889:8889"         # Prometheus EXPORTER metrics: endpoint where services metrics are sent then scraped by the prometheus service to monitor instrumented services.
      - "13133:13133"       # health_check extension
      - "4317:4317"         # OTLP gRPC receiver
      - "4318:4318"         # OTLP HTTP receiver
      - "55679:55679"       # zpages extension

  # Loki to collect logs
  loki:
    image: grafana/loki:main-f80d68a
    container_name: loki
    ports:
      - "3100:3100"        # HTTP port to receive logs data from the OTEL Collector
    restart: on-failure
    volumes:
      - ./loki/loki-config.yaml:/etc/loki/loki-config.yaml
    command: [ "--config.file=/etc/loki/loki-config.yaml" ]
     
  prometheus:
    build:
      context: ./prometheus
      dockerfile: Dockerfile
    container_name: prometheus
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'  
      - '--query.lookback-delta=5s'
    ports:
      - "9090:9090"
    restart: on-failure
    depends_on:
      - collector-gateway
  
  grafana:
    image: grafana/grafana-enterprise # :10.3.10
    container_name: grafana
    restart: unless-stopped
    ports:
      - "3010:3000"
    volumes:
      - ./grafana-storage:/var/lib/grafana


  # The Tempo service stores traces send to it by Grafana Alloy, and takes
  # queries from Grafana to visualise those traces.
  tempo:
    image: grafana/tempo:main-ce317a8
    ports:
      - "3200:3200"
      - "4327:4317"
      - "4328:4318"
      - "9411:9411"
      - "55680:55680"
      - "55681:55681"
      - "14250:14250"
    command: [ "-config.file=/etc/tempo.yaml" ]
    volumes:
      - "./tempo/tempo.yaml:/etc/tempo.yaml"

  # Generate fake traces...
  k6-tracing:
    image: ghcr.io/grafana/xk6-client-tracing:v0.0.5
    environment:
      - ENDPOINT=collector-gateway:4317
    restart: always
    depends_on:
      - collector-gateway  

  mimir:
    image: grafana/mimir:2.13.0
    command: ["-ingester.native-histograms-ingestion-enabled=true", "-config.file=/etc/mimir.yaml"]
    ports:
      - "9009:9009"
    volumes:
      - "./mimir/mimir.yaml:/etc/mimir.yaml"  