input {
  # file {
  #   type => "apigateway-AI-ECommerce"
  #   mode => "read"  # Read mode to process the entire file
  #   path => "/usr/share/logstash/ingest_data/AI-ECommerce-Output.csv"
  #   start_position => "beginning"  # Start reading from the beginning of the file
  # }

  file {
    type => "apigateway-AI-ECommerce"
    mode => "tail"
    path => "/usr/share/logstash/ingest_data/AI-ECommerce-Output.csv"
  }

  # file {
  #   type => "usersvc"
  #   mode => "tail"
  #   path => "/usr/share/logstash/ingest_data/userservice.csv"
  # }
  # file {
  #   type => "hardware"
  #   mode => "tail"
  #   path => "/usr/share/logstash/ingest_data/*Hardware.log"
  # }
}

# 2024-11-14 08:51:14.689, info, error_type, apigateway, /userslist, Users list fetched successfully from user-service in 11 ms

filter {
  
  # mutate {
  #     remove_field => ["@timestamp", "host"]
  # }

  if [type] == "apigateway-AI-ECommerce" {
    csv {
      separator => ","  
      columns => ["DateTime","Severity","EpochTime","ErrorType","Service","EndPoint","DataVal1","DataVal2","Message"]
    }
  } else if [type] == "usersvc" {
    csv {
      separator => ","  
      columns => ["DateTime","Severity","Service","EndPoint","Message"]
    }
  }

  if [type] == "apigateway-AI-ECommerce" {

    if ![DataVal1] or [DataVal1] == "" {
        mutate { replace => { "DataVal1" => "" } }
    }
    if ![DataVal2] or [DataVal2] == "" {
      mutate { replace => { "DataVal2" => "" } }
    }
    if ![Message] or [Message] == "" {
      mutate { replace => { "Message" => "" } }
    }
    
    http {
      url => "http://servemodel_fast_api:8088/addentry" 
      verb => "POST"  # HTTP method, can be GET, POST, etc.
      query => {
        "date" => "%{[DateTime]}"  
        "sever" => "%{[Severity]}"
        "epoch" => "%{[EpochTime]}"
        "error_type" => "%{[ErrorType]}"
        "service" => "%{[Service]}"
        "endpoint" => "%{[EndPoint]}"
        "data1" => "%{[DataVal1]}"
        "data2" => "%{[DataVal2]}"
        "message" => "%{[Message]}"   
      }
      target_body => "req_answer_body"  # Store the HTTP response body in this field
      target_headers => "req_answer_header"  # Optionally store response headers
    }

    # # Parse the JSON response body into fields
    # json {
    #   source => "req_answer_body"
    #   target => "req_answer_body_parsed"
    # }

    # # Check if Alarm is True and add your logic
    # if [req_answer_body_parsed][Alarm] == "True" {
    #   mutate {
    #     add_tag => ["alarm_triggered"]
    #   }
    # }

    # http {
    #   url => "http://servemodel_fast_api:8088/" 
    #   verb => "GET"  # HTTP method, can be GET, POST, etc.
    #   # query => {
    #   #   "param" => "%{some_field}"  # You can use fields from your log as query parameters
    #   # }
    #   target_body => "req_answer_body"  # Store the HTTP response body in this field
    #   target_headers => "req_answer_header"  # Optionally store response headers
    # }
    
    # curl -X POST "http://127.0.0.1:8088/additem?itemname=pink&nb=20"
    # http {
    #   url => "http://servemodel_fast_api:8088/additem" 
    #   verb => "POST"  # HTTP method, can be GET, POST, etc.
    #   query => {
    #     "itemname" => "Value 1"     
    #     "nb" => "%{[DataVal1]}"     
    #   }
    #   target_body => "req_answer_body"  # Store the HTTP response body in this field
    #   target_headers => "req_answer_header"  # Optionally store response headers
    # }

  }

  # Assuming your logs have a timestamp field (e.g., 'log_timestamp'),
  # use the date filter to parse and convert it into @timestamp
  # date {
  #   match => ["DateTime", "yyyy-MM-dd HH:mm:ss.SSS"]  # Adjust the format to match your log timestamps
  #   target => "@timestamp"
  # }

  # Calculate the time difference and filter logs from the last minute
  # ruby {
  #   code => "
  #     time_now = Time.now.to_i
  #     event_time = event.get('@timestamp').to_i
  #     time_difference = time_now - event_time
  #     event.cancel if time_difference > 60  
  #   "
  # }
  
}

output {

  # Send logs to Elasticsearch
  elasticsearch {
    index => "logstash-%{+YYYY.MM.dd}"
    hosts=> "${ELASTIC_HOSTS}"
    user=> "${ELASTIC_USER}"
    password=> "${ELASTIC_PASSWORD}"
    cacert=> "certs/ca/ca.crt"
  }
  
  # Ouput for debugging
  # stdout { 
  #   codec => rubydebug 
  # }

  # Additional output to a single log file
  # file {
  #   path => "/usr/share/logstash/ingest_data/system_unified_log.csv"
  #   codec => "line"     #"json_lines"  # Or "line" if you prefer plain text
  # }
}
